# Mini Warehouse ML ‚Äì End-to-End Data Engineering Pipeline

Kompleksowy projekt Data Engineering + ML z pe≈Çnym pipeline‚Äôem ETL dzia≈ÇajƒÖcym w stylu produkcyjnym:

- **Apache Airflow** ‚Äì orkiestracja
- **MinIO (S3)** ‚Äì storage plik√≥w
- **PostgreSQL** ‚Äì hurtownia danych w warstwach `bronze / silver / gold / ml`
- **Python + Pandas + scikit-learn + XGBoost** ‚Äì transformacje i modele
- **SQL** ‚Äì walidacja, czyszczenie, feature engineering
- **Docker Compose** ‚Äì pe≈Çna infrastruktura lokalna
- **Artifacts** ‚Äì zapisywanie modeli, metryk, SHAP i predykcji

Ca≈Ço≈õƒá jest zaprojektowana jako **portfolio-quality project**.

---

## Spis tre≈õci

1. [Architektura](#architektura)
2. [Struktura repozytorium](#struktura-repozytorium)
3. [Warstwy hurtowni danych](#warstwy-hurtowni-danych)
4. [ERD ‚Äì zale≈ºno≈õci miƒôdzy tabelami (Mermaid)](#erd--zale≈ºno≈õci-miƒôdzy-tabelami-mermaid)
5. [Setup ‚Äì ≈õrodowisko lokalne](#setup--≈õrodowisko-lokalne)
   - [Krok 1 ‚Äì instalacje zale≈ºno≈õci](#krok-1--instalacje-zale≈ºno≈õci)
   - [Krok 2 ‚Äì plik `.env`](#krok-1--plik-env)
   - [Krok 3 ‚Äì generowanie kluczy](#krok-2--generowanie-kluczy)
   - [Krok 4 ‚Äì uruchomienie Docker Compose](#krok-3--uruchomienie-docker-compose)
7. [DAG Airflow ‚Äì warehouse_daily](#dag-airflow--warehouse_daily)
8. [Warstwa ML ‚Äì trenowanie modeli i predykcje](#warstwa-ml--trenowanie-modeli-i-predykcje)
   - [Trenowanie i wyb√≥r najlepszego modelu](#trenowanie-i-wyb√≥r-najlepszego-modelu)
   - [Feature importance](#feature-importance)
   - [SHAP ‚Äì interpretowalno≈õƒá modelu](#shap--interpretowalno≈õƒá-modelu)
   - [Predykcje nowych mieszka≈Ñ](#predykcje-nowych-mieszka≈Ñ)
9. [Artefakty ML ‚Äì co zostaje zapisane](#artefakty-ml--co-zostaje-zapisane)
10. [Restart / zatrzymanie ≈õrodowiska](#restart--zatrzymanie-≈õrodowiska)

---

## Architektura

**High-level:**

```text
Raw CSV
  ‚Üì
Bronze (Pandas ‚Üí Parquet ‚Üí MinIO)
  ‚Üì
Silver (SQL: typing, cleaning, validation)
  ‚Üì
Gold (Feature engineering, KPI views, dane do ML)
  ‚Üì
ML (trenowanie modeli, feature importance, SHAP, predykcje)

# üìÅ Struktura repozytorium
```
Ca≈Ço≈õciƒÖ steruje DAG Airflow: warehouse_daily.

## Struktura repozytorium
```
mini-warehouse-ml/
‚îú‚îÄ‚îÄ docker-compose.airflow.yml
‚îú‚îÄ‚îÄ docker-compose.yml
‚îÇ
‚îú‚îÄ‚îÄ airflow/                 # konfiguracja Airflow + docker-compose dla orkiestracji
‚îÇ   ‚îú‚îÄ‚îÄ dags/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ warehouse_dag.py # g≈Ç√≥wny DAG: bronze ‚Üí silver ‚Üí gold ‚Üí ML
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ __pycache__
‚îÇ   ‚îÇ	‚îÇ   ‚îî‚îÄ‚îÄ warehouse_dag.cpython-312.pyc
‚îÇ   ‚îú‚îÄ‚îÄ logs
‚îÇ   ‚îÇ	‚îî‚îÄ‚îÄ dag_processor_manager
‚îÇ   ‚îÇ	‚îî‚îÄ‚îÄ scheduler
‚îÇ   ‚îÇ	‚îî‚îÄ‚îÄ dag_id=warehouse_daily
‚îÇ   ‚îú‚îÄ‚îÄ home
‚îÇ
‚îú‚îÄ‚îÄ etl/                     # ETL w Pythonie (bronze + integracja z MinIO)
‚îÇ   ‚îú‚îÄ‚îÄ extract.py           # CSV ‚Üí MinIO
‚îÇ   ‚îú‚îÄ‚îÄ transform.py         # pandas ‚Üí Parquet
‚îÇ   ‚îî‚îÄ‚îÄ load.py              # Parquet ‚Üí Postgres (bronze.housing_raw)
‚îÇ   ‚îî‚îÄ‚îÄ load_raw.py 		 # Surowy plik csv -> ca≈Çy proces "ETL" po stronie SQL
‚îÇ
‚îú‚îÄ‚îÄ SQL/                     # SQL dla warstw silver / gold / walidacji
‚îÇ   ‚îú‚îÄ‚îÄ SQL_raw/			# SQL-owy ETL 
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 00_discovery/		# ekspolarcja danych raw
‚îÇ   ‚îÇ	‚îÇ   ‚îî‚îÄ‚îÄ 010_schema_overview.sql
‚îÇ   ‚îÇ	‚îÇ   ‚îî‚îÄ‚îÄ 020_null_heatmap.sql
‚îÇ   ‚îÇ	‚îÇ   ‚îî‚îÄ‚îÄ 030_basix_statistics.sql
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 01_staging/			# czyszczenie i standaryzacja			
‚îÇ   ‚îÇ	‚îÇ   ‚îî‚îÄ‚îÄ 110_handle_missing_values.sql
‚îÇ   ‚îÇ	‚îÇ   ‚îî‚îÄ‚îÄ 120_cast_and_normalize.sql
‚îÇ   ‚îÇ	‚îÇ   ‚îî‚îÄ‚îÄ 130_handle_logic.sql
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 02_gold/			# features, KPIs, outliers
‚îÇ   ‚îÇ	‚îÇ   ‚îî‚îÄ‚îÄ 210_gold_feature.sql
‚îÇ   ‚îÇ	‚îÇ   ‚îî‚îÄ‚îÄ 220_gold_valid.sql
‚îÇ   ‚îÇ	‚îÇ   ‚îî‚îÄ‚îÄ 230_gold_invalid.sql
‚îÇ   ‚îÇ	‚îÇ   ‚îî‚îÄ‚îÄ 240_price_city_daily.sql
‚îÇ   ‚îÇ	‚îÇ   ‚îî‚îÄ‚îÄ 250_gold_outliers_iqr.sql
‚îÇ   ‚îÇ	‚îÇ   ‚îî‚îÄ‚îÄ 260_gold_duplicates.sql
‚îÇ   ‚îÇ	‚îÇ   ‚îî‚îÄ‚îÄ 270_gold_kpi.sql
‚îÇ   ‚îú‚îÄ‚îÄ SQL_after_etl/          # SQL po Python ETL
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 01_quality_checks.sql
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 02_standarize_types.sql
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 03_outliers_check.sql
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 04_buisness_logic_cleaning.sql
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 05_final_view.sql 
‚îÇ
‚îú‚îÄ‚îÄ ml/                      # czƒô≈õƒá ML
‚îÇ   ‚îú‚îÄ‚îÄ ml_final.py          # trenowanie modeli + wyb√≥r najlepszego + log run√≥w
‚îÇ   ‚îú‚îÄ‚îÄ feature_importance.py# wykres wa≈ºno≈õci cech
‚îÇ   ‚îú‚îÄ‚îÄ shap_explainer.py    # SHAP ‚Äì interpretacja modelu
‚îÇ   ‚îî‚îÄ‚îÄ predict_sample.py    # predykcje nowych mieszka≈Ñ + zapis do DB i Excela
‚îÇ   ‚îî‚îÄ‚îÄ __pycache__
‚îÇ   ‚îî‚îÄ‚îÄ artifacts
‚îÇ
‚îú‚îÄ‚îÄ artifacts/               # wyj≈õcia z ML (modele, metryki, wykresy, predykcje)
‚îÇ   ‚îî‚îÄ‚îÄ best_model_*.joblib
‚îÇ   ‚îî‚îÄ‚îÄ feature_importance.csv
‚îÇ   ‚îî‚îÄ‚îÄ feature_importance.png
‚îÇ   ‚îî‚îÄ‚îÄ model_metrics.csv
‚îÇ   ‚îî‚îÄ‚îÄ predictions_YYYY_MMDD_****.xlsx
‚îÇ   ‚îî‚îÄ‚îÄ shap_summary.png
‚îÇ   ‚îî‚îÄ‚îÄ shap_values.csv
‚îÇ
‚îú‚îÄ‚îÄ data/                    # dane wej≈õciowe 
‚îÇ   ‚îî‚îÄ‚îÄ raw
‚îÇ      ‚îî‚îÄ‚îÄ housing_10k_sample.csv
‚îÇ      ‚îî‚îÄ‚îÄ housing_800k.csv
‚îÇ
‚îú‚îÄ‚îÄ notebooks/               # praca eksploracyjna
‚îÇ   ‚îî‚îÄ‚îÄ explore_raw.ipynb
‚îÇ
‚îú‚îÄ‚îÄ warehouse/               # dodatkowe materia≈Çy dot. ERD
‚îÇ   ‚îî‚îÄ‚îÄ erd.md
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt         # zale≈ºno≈õci Pythona (ML + lokalne skrypty)
‚îî‚îÄ‚îÄ README.md
```
### Warstwy hurtowni danych

**Bronze ‚Äî Raw Layer**

Warstwa **BRONZE** zawiera dane ‚Äûas-is‚Äù, w formie najbardziej zbli≈ºonej do ≈∫r√≥d≈Ça, bez walidacji i bez typowania.
Zawarto≈õƒá:
- bronze.housing_raw
Cechy:
- ‚úî brak typ√≥w
- ‚úî brak walidacji
- ‚úî pe≈Çne dane surowe

‚∏ª

**Silver ‚Äî Clean Layer**

Warstwa **SILVER** zawiera dane oczyszczone, otagowane typami oraz gotowe do dalszego wzbogacania.
Zawarto≈õƒá (views):
- silver.housing_clean ‚Äì dane po walidacji, usuniƒôte warto≈õci b≈Çƒôdne
- silver.housing_typed ‚Äì ujednolicone typy, poprawione formaty dat/liczb
Cechy transformacji:
- usuwanie b≈Çƒôdnych rekord√≥w
- konwersja typ√≥w
- normalizacja kolumn
- wstƒôpne ≈ÇƒÖczenie danych

‚∏ª

**Gold ‚Äî Feature Layer**

Warstwa **GOLD** jest u≈ºywana do analiz biznesowych i trenowania modeli ML.
Tabele:
- gold.housing_features ‚Äì g≈Ç√≥wna tabela cech numerycznych i kategorycznych
- gold.outliers_iqr ‚Äì wykryte obserwacje odstajƒÖce
- gold.price_city_daily ‚Äì dzienne statystyki cenowe per miasto
Widoki:
- gold.clean ‚Äì dane przefiltrowane, przygotowane do dalszej analizy
- gold.housing_valid ‚Äì ostateczny zbi√≥r treningowo-walidacyjny dla modeli ML

‚∏ª

### ML ‚Äî Model Predictions & Metadata

Schemat ML przechowuje wyniki predykcji oraz metadane trening√≥w modeli.
- ml.housing_predictions
Predykcje wygenerowane przez najlepszy model:
- listing_id
- predicted_price_total
- scored_at
- model_path
- ml.model_runs
Log ka≈ºdego treningu modelu:
- run_id
- model_name
- mae, rmse, r2
- train_rows, valid_rows
- scored_at
- pipeline_sha (hash pliku modelu)
---

### Podsumowanie warstw hurtowni danych

| Warstwa | Typ     | Obiekty                                                                 | Cel                                   |
|---------|---------|--------------------------------------------------------------------------|----------------------------------------|
| Bronze  | tabela  | `bronze.housing_raw`                                                     | dane surowe, ≈∫r√≥d≈Çowe                  |
| Silver  | widoki  | `silver.housing_clean`, `silver.housing_typed`                           | czyszczenie, typowanie                 |
| Gold    | tabele  | `gold.housing_features`, `gold.outliers_iqr`, `gold.price_city_daily`    | cechy, agregacje, statystyki           |
| Gold    | widoki  | `gold.clean`, `gold.housing_valid`                                       | finalne dane do ML                     |
| ML      | tabele  | `ml.housing_predictions`, `ml.model_runs`                                | predykcje i metadane modeli            |


##  ERD ‚Äì zale≈ºno≈õci miƒôdzy tabelami (Mermaid)

```mermaid
flowchart TD

  subgraph BRONZE ["Bronze ‚Äì Raw"]
    B1["bronze.housing_raw"]
  end

  subgraph SILVER ["Silver ‚Äì Clean"]
    S1["silver.housing_typed"]
    S2["silver.housing_clean"]
  end

  subgraph GOLD ["Gold ‚Äì Features & Views"]
    G1["gold.housing_features"]
    G2["gold.outliers_iqr"]
    G3["gold.price_city_daily"]
    GV1["gold.clean"]
    GV2["gold.housing_valid"]
  end

  subgraph ML ["ML ‚Äì Predictions & Runs"]
    M1["ml.housing_predictions"]
    M2["ml.model_runs"]
  end

  %% przep≈Çyw danych
  B1 --> S1 --> S2 --> G1
  G1 --> GV1 --> GV2
  G1 --> G2
  G1 --> G3

  %% dane do ML
  GV2 --> M1
  GV2 --> M2
```

## Setup ‚Äì ≈õrodowisko lokalne

**Wymagania**
	‚Ä¢	Docker + Docker Compose
	‚Ä¢	Python 3.10‚Äì3.12 (do lokalnego uruchamiania skrypt√≥w ML)
	‚Ä¢	PostgreSQL lokalnie (je≈õli chcesz podglƒÖdaƒá dane poza kontenerem)

### Krok 1 ‚Äì instalacje zale≈ºno≈õci
**Instalacja zale≈ºno≈õci Python (dla lokalnego uruchamiania ML)**

W celu uruchomienia skrypt√≥w lokalnie, nale≈ºy wykonaƒá komende poni≈ºej:
```bash
python3 -m venv .venv
source .venv/bin/activate        # macOS/Linux
# lub
.venv\Scripts\activate           # Windows

pip install -r requirements.txt
```


### Krok 2 ‚Äì plik .env

W katalogu g≈Ç√≥wnym projektu:
```bash
cp airflow/.env.example airflow/.env
nano airflow/.env
```
Uzupe≈Çnij warto≈õci:

**Sekcja bezpiecze≈Ñstwa:**
```bash
AIRFLOW__CORE__FERNET_KEY=<WSTAW_TUTAJ_FERNET_KEY>
AIRFLOW__WEBSERVER__SECRET_KEY=<WSTAW_TUTAJ_SECRET_KEY>
```

**Dane logowania do Airflow:**
```bash
_AIRFLOW_WWW_USER_CREATE=True
_AIRFLOW_WWW_USER_USERNAME=admin
_AIRFLOW_WWW_USER_PASSWORD=admin123
```

**Po≈ÇƒÖczenie do Postgresa z DAG-a:**
```bash
PG_HOST=host.docker.internal
PG_PORT=5432
PG_USER=postgres
PG_PASSWORD=postgres
PG_DB=warehouse
```

**MinIO:**
```bash
MINIO_ROOT_USER=admin
MINIO_ROOT_PASSWORD=admin12345
S3_ENDPOINT=http://host.docker.internal:9000
```

### Krok 3 ‚Äì generowanie kluczy

**Fernet Key:**
```bash
python3 - <<'EOF'
from cryptography.fernet import Fernet
print(Fernet.generate_key().decode())
EOF
```

**Secret Key:**
```bash
openssl rand -hex 64
```

Wygenerowane warto≈õci nale≈ºy wkleiƒá do airflow/.env.

### Krok 4 - uruchomienie Docker Compose 

```bash
docker compose -f docker-compose.airflow.yml up -d
```

Airflow UI bƒôdzie dostƒôpny pod adresem:
*http://localhost:8081*

Dane logowania:
```bash
Username: admin
Password: admin123
```

### Trouble shooting - problemy z logowaniem do Airflow

Je≈õli domy≈õlne logowanie nie dzia≈Ça (np. po zmianie `_AIRFLOW_WWW_USER_*` albo pierwszym starcie kontenera), mo≈ºna zresetowaƒá has≈Ço z poziomu Dockera:

1. Wej≈õcie do konternera Airflow:
```bash
docker compose -d docker-compose.airflow.yml exec airflow bash
```

2. Uruchamianie skryptu zmiany has≈Ça:
```bash
airflow users reset-password -u admin
```
Nale≈ºy podac nowe has≈Ço, a nastƒôpnie je powt√≥rzyƒá.

3. Wyj≈õcie z kontera:
```bash
exit
```

## DAG Airflow ‚Äì warehouse_daily

Po w≈ÇƒÖczeniu DAG-a warehouse_daily w UI Airflow, pipeline wykona kolejno:
1.	**Extract** ‚Äì wrzucenie lokalnego CSV do MinIO
2.	**Transform** ‚Äì przetworzenie do Parquet (Pandas)
3.	**Load** ‚Äì import do Postgresa: bronze.housing_raw
4.	**Silver** ‚Äì handle_missing_values ‚Äì uzupe≈Çnianie/obs≈Çuga brak√≥w
5.	**Silver** ‚Äì cast_and_normalize ‚Äì typowanie i normalizacja p√≥l
6.	**Silver** ‚Äì logic checks ‚Äì regu≈Çy biznesowe i jako≈õciowe
7.	**Gold** ‚Äì features ‚Äì tworzenie cech (m.in. floor_ratio, season, area_sqm_bucket)
8.	**Gold** ‚Äì valid/invalid/duplicates ‚Äì widoki kontrolne
9.	**ML** ‚Äì train_models ‚Äì trenowanie modeli i zapis artefakt√≥w
10.	**ML** ‚Äì feature_importance_ml ‚Äì generacja wykresu wa≈ºno≈õci cech
11.	**ML** ‚Äì shap_explainer_ml ‚Äì obliczenie SHAP i zapis wykresu
12.	**ML** ‚Äì Model_prediction_sample ‚Äì predykcja nowych mieszka≈Ñ + zapis do DB i Excela

## Warstwa ML ‚Äì trenowanie modeli i predykcje

### **Trenowanie i wyb√≥r najlepszego modelu**

Skrypt: ml/ml_final.py

Modele:
- RandomForestRegressor
- GradientBoostingRegressor
- XGBRegressor

Wykonywane kroki:
1.	Pobranie danych z gold.housing_valid
2.	Podzia≈Ç na train / valid / test
3.	Zbudowanie preprocessora (ColumnTransformer ‚Äì num + cat)
4.	RandomizedSearchCV dla ka≈ºdego modelu (MAE jako metryka)
5.	Wyb√≥r najlepszego modelu (najni≈ºsze MAE na valid)
6.	Zapis pipeline‚Äôu do artifacts/best_model_<Model>.joblib
7.	Wyliczenie SHA256 pipeline‚Äôu i zapis do ml.model_runs

### **Feature importance**

Skrypt: ml/feature_importance.py
- ≈Çaduje best_model_*.joblib z katalogu artifacts/
- wyciƒÖga feature_importances_
- zapisuje:
	- artifacts/feature_importance.csv
	- artifacts/feature_importance_top20.png ‚Äì wykres TOP 20 cech

### **SHAP ‚Äì interpretowalno≈õƒá modelu**

Skrypt: ml/shap_explainer.py
- pobiera pr√≥bkƒô danych z gold.housing_valid
- u≈ºywa TreeExplainer (SHAP) dla najlepszego modelu
- zapisuje:
	- artifacts/shap_values.csv
	- artifacts/shap_summary.png ‚Äì wykres mean(|SHAP value|) dla top cech

### **Predykcje nowych mieszka≈Ñ**

Skrypt: ml/predict_sample.py
1.	Pobiera N losowych rekord√≥w z gold.housing_valid
2.	Odrzuca kolumnƒô price_total (target)
3.	Wylicza predykcje predicted_price_total przy u≈ºyciu najlepszego modelu
4.	Zapisuje wyniki do:
	- Postgres: ml.housing_predictions
	- Excela: artifacts/predictions_<YYYYMMDD_HHMM>.xlsx

Przyk≈Çadowe kolumny:
- listing_id
- predicted_price_total
- scored_at (UTC)
- model_path (np. best_model_RandomForest.joblib)

## Artefakty ML ‚Äì co zostaje zapisane

W katalogu artifacts/ powiniene≈õ zobaczyƒá m.in.:
**Modele i metryki:**
- best_model_<Model>.joblib
- model_metrics.csv
- feature_importance.csv
- feature_importance_top20.png
**SHAP:**
- shap_values.csv
- shap_summary.png
**Predykcje**:
- predictions_<YYYYMMDD_HHMM>.xlsx

## Restart / zatrzymanie ≈õrodowiska

Zatrzymanie kontener√≥w:
```bash
docker compose -d docker-compose.airflow.yml down
```

Uruchomienie ponowne:
```bash
docker compose -d docker-compose.airflow.yml up -d
```
---

## Autor

Projekt zosta≈Ç w ca≈Ço≈õci zaprojektowany, zaimplementowany i udokumentowany przez:

**Mateusz Wykowski**  
Data Engineer & ML Enthusiast

Repozytorium stanowi element portfolio i pokazuje praktycznƒÖ implementacjƒô:
- end-to-end pipeline'u ETL w architekturze bronze ‚Üí silver ‚Üí gold,
- orkiestracji z u≈ºyciem Apache Airflow,
- integracji z MinIO (S3),
- przetwarzania danych w Pythonie i SQL,
- pe≈Çnego cyklu ML: preprocessing, trenowanie, ewaluacja, SHAP, predykcje,
- oraz projektowych dobrych praktyk Data Engineering.

Kontakt: **MateusWykowski@gmail.com**

---
