# ğŸ—ï¸ Mini Warehouse ML â€“ End-to-End Data Engineering Pipeline

Kompleksowy projekt Data Engineering z peÅ‚nym pipeline'em ETL dziaÅ‚ajÄ…cym w stylu produkcyjnym:

- Apache Airflow â€” orkiestracja
- MinIO (S3) â€” storage plikÃ³w
- PostgreSQL â€” baza danych bronze/silver/gold
- Python + Pandas â€” transformacje
- SQL â€” walidacja, czyszczenie i feature engineering
- Docker Compose â€” peÅ‚na infrastruktura lokalna

CaÅ‚oÅ›Ä‡ jest zaprojektowana jako portfolio-quality project.

---

# ğŸ“‚ Architektura

  Raw CSV â†’ Bronze (Pandas â†’ Parquet â†’ MinIO)
                     â†“
  Silver (SQL: typing, cleaning, validation)
                     â†“
  Gold (Feature engineering, KPI views, modeling)

  DAG Airflow (`warehouse_daily`) kontroluje wszystkie kroki.

---

# ğŸ“ Struktura repozytorium

```
mini-warehouse-ml/
â”‚
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â””â”€â”€ warehouse_dag.py
â”‚   â”œâ”€â”€ repo/
â”‚   â”‚   â”œâ”€â”€ etl/
â”‚   â”‚   â”‚   â”œâ”€â”€ extract.py
â”‚   â”‚   â”‚   â”œâ”€â”€ transform.py
â”‚   â”‚   â”‚   â””â”€â”€ load.py
â”‚   â”‚   â””â”€â”€ SQL/
â”‚   â”‚       â”œâ”€â”€ SQL_raw/
â”‚   â”‚       â”‚   â””â”€â”€ 01_staging/
â”‚   â”‚       â”œâ”€â”€ SQL_silver/
â”‚   â”‚       â””â”€â”€ SQL_gold/
â”‚   â”œâ”€â”€ .env.example
â”‚   â””â”€â”€ docker-compose.airflow.yml
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ housing_800k.csv
â”‚
â””â”€â”€ README.md
```

---

# âš™ï¸ Instalacja i uruchomienie

PoniÅ¼ej instrukcja 1:1, aby kaÅ¼dy mÃ³gÅ‚ powtÃ³rzyÄ‡ projekt.

---

## 1ï¸âƒ£ UtwÃ³rz plik `.env`

W katalogu `airflow/`:

```bash
cp airflow/.env.example airflow/.env
```

Edytuj:
```bash
nano airflow/.env
```

## 2ï¸âƒ£ UzupeÅ‚nij .env swoimi wartoÅ›ciami

# === SECURITY ===
AIRFLOW__CORE__FERNET_KEY=<WSTAW_TUTAJ_FERNET_KEY>
AIRFLOW__WEBSERVER__SECRET_KEY=<WSTAW_TUTAJ_SECRET_KEY>

# === AIRFLOW ADMIN ===
_AIRFLOW_WWW_USER_CREATE=True
_AIRFLOW_WWW_USER_USERNAME=admin
_AIRFLOW_WWW_USER_PASSWORD=admin123

# === POSTGRES CONNECTION FOR DAG ===
PG_HOST=host.docker.internal
PG_PORT=5432
PG_USER=postgres
PG_PASSWORD=postgres
PG_DB=warehouse

# === MINIO ===
MINIO_ROOT_USER=admin
MINIO_ROOT_PASSWORD=admin12345
S3_ENDPOINT=http://host.docker.internal:9000

## 3ï¸âƒ£ Wygeneruj klucze

Fernet Key:
```bash
python3 - <<'EOF'
from cryptography.fernet import Fernet
print(Fernet.generate_key().decode())
EOF
```

Secret Key:
```bash
openssl rand -hex 64
```

Wklej oba do .env.

## 4ï¸âƒ£ Uruchom Å›rodowisko

```bash
docker compose -f docker-compose.airflow.yml up -d
```

Airflow UI:
ğŸ‘‰ http://localhost:8081

Login:
```bash
Username: admin
Password: admin123
```

## â–¶ï¸ Uruchamianie DAG-a

W Airflow aktywuj DAG:

**warehouse_daily**

Pipeline wykona kolejno:
	1.	Extract â†’ upload CSV to MinIO
	2.	Transform â†’ Parquet
	3.	Load â†’ Postgres bronze
	4.	Silver typing & missing value handling
	5.	Silver logic checks
	6.	Gold feature engineering
	7.	Gold validation views

## ğŸ—„ï¸ Warstwy bazy danych

**Bronze**
	â€¢	bronze.housing_raw

**Silver**
	â€¢	silver.housing_typed
	â€¢	silver.housing_clean

**Gold**
	â€¢	gold.housing_features
	â€¢	gold.kpi_overview
	â€¢	gold.housing_valid
	â€¢	gold.housing_invalid
	â€¢	gold.duplicates

## ğŸ§  Feature Engineering (Gold)

Cechy wyliczane:
	â€¢	floor_ratio
	â€¢	decade
	â€¢	season
	â€¢	building_age
	â€¢	area_sqm_bucket
	â€¢	distance_km_bucket
	â€¢	listing_day_of_week
	â€¢	has_elevator_int


Kod SQL znajduje siÄ™ w:
```bash
airflow/repo/SQL/SQL_gold/
```

## ğŸ”„ Restart Å›rodowiska

Stop:
```bash
docker compose -f docker-compose.airflow.yml down
```

Start:
```bash
docker compose -f docker-compose.airflow.yml up -d
```
